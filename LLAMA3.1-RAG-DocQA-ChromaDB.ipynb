{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35dcc55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.6.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: langchain-chroma in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: langchain in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.3.18)\n",
      "Requirement already satisfied: langchain-text-splitters in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: langchain-groq in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.2.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.43.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: unstructured in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (2.10.6)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (3.17.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (4.65.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.70.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (6.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-chroma) (0.3.40)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-groq) (0.18.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: chardet in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (4.0.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (4.9.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (3.8.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (0.8.10)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (4.12.2)\n",
      "Requirement already satisfied: emoji in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (2.14.1)\n",
      "Requirement already satisfied: python-iso639 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (2025.2.18)\n",
      "Requirement already satisfied: langdetect in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (3.12.1)\n",
      "Requirement already satisfied: backoff in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: unstructured-client in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (0.30.6)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (1.14.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (5.9.0)\n",
      "Requirement already satisfied: onnx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (1.17.0)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (1.17.0)\n",
      "Requirement already satisfied: pdfminer.six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (20231228)\n",
      "Requirement already satisfied: pikepdf in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (9.5.2)\n",
      "Requirement already satisfied: pillow-heif in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (0.21.0)\n",
      "Requirement already satisfied: pypdf in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (5.3.0)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (0.3.13)\n",
      "Requirement already satisfied: google-cloud-vision in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (3.10.0)\n",
      "Requirement already satisfied: effdet in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (0.4.1)\n",
      "Requirement already satisfied: unstructured-inference==0.7.36 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (0.7.36)\n",
      "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured) (0.3.13)\n",
      "Requirement already satisfied: layoutparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured-inference==0.7.36->unstructured) (0.3.4)\n",
      "Requirement already satisfied: python-multipart in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured-inference==0.7.36->unstructured) (0.0.20)\n",
      "Requirement already satisfied: opencv-python!=4.7.0.68 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured-inference==0.7.36->unstructured) (4.11.0.86)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured-inference==0.7.36->unstructured) (3.7.2)\n",
      "Requirement already satisfied: timm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured-inference==0.7.36->unstructured) (1.0.15)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.45.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2025.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.58.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.26.16)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.68.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.30.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.51b0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.51b0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.51b0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4->unstructured) (2.4)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\anaconda3\\lib\\site-packages (from effdet->unstructured) (0.21.0)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from effdet->unstructured) (2.0.8)\n",
      "Requirement already satisfied: omegaconf>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from effdet->unstructured) (2.3.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-cloud-vision->unstructured) (2.24.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-cloud-vision->unstructured) (1.26.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk->unstructured) (1.2.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pdfminer.six->unstructured) (41.0.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured-client->unstructured) (24.1.0)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured-client->unstructured) (0.2.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured) (1.15.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured) (1.70.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.11.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma) (2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from omegaconf>=2.0->effdet->unstructured) (4.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->unstructured-inference==0.7.36->unstructured) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->unstructured-inference==0.7.36->unstructured) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->unstructured-inference==0.7.36->unstructured) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->unstructured-inference==0.7.36->unstructured) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->unstructured-inference==0.7.36->unstructured) (3.0.9)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from layoutparser->unstructured-inference==0.7.36->unstructured) (2.0.3)\n",
      "Requirement already satisfied: iopath in c:\\users\\hp\\anaconda3\\lib\\site-packages (from layoutparser->unstructured-inference==0.7.36->unstructured) (0.1.10)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\hp\\anaconda3\\lib\\site-packages (from layoutparser->unstructured-inference==0.7.36->unstructured) (0.11.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured) (2.21)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Requirement already satisfied: portalocker in c:\\users\\hp\\anaconda3\\lib\\site-packages (from iopath->layoutparser->unstructured-inference==0.7.36->unstructured) (3.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->layoutparser->unstructured-inference==0.7.36->unstructured) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->layoutparser->unstructured-inference==0.7.36->unstructured) (2023.3)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pdfplumber->layoutparser->unstructured-inference==0.7.36->unstructured) (4.30.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from portalocker->iopath->layoutparser->unstructured-inference==0.7.36->unstructured) (305.1)\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb langchain-chroma langchain langchain-community langchain-text-splitters langchain-groq transformers sentence-transformers unstructured unstructured[pdf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70fe6125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-kerasNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for tf-keras from https://files.pythonhosted.org/packages/8a/ed/e08afca471299b04a34cd548e64e89d0153eda0e6cf9b715356777e24774/tf_keras-2.18.0-py3-none-any.whl.metadata\n",
      "  Using cached tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<2.19,>=2.18 (from tf-keras)\n",
      "  Obtaining dependency information for tensorflow<2.19,>=2.18 from https://files.pythonhosted.org/packages/cf/24/271e77c22724f370c24c705f394b8035b4d27e4c2c6339f3f45ab9b8258e/tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.0)\n",
      "Using cached tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl (7.5 kB)\n",
      "Installing collected packages: tensorflow, tf-keras\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "Successfully installed tensorflow-2.18.0 tf-keras-2.18.0\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36440fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optree in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.12.1)\n",
      "Collecting optree\n",
      "  Obtaining dependency information for optree from https://files.pythonhosted.org/packages/af/c2/811b76e321b3a83828fa63da17e3409d577ce7d0366a601d913eaeb49679/optree-0.14.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading optree-0.14.0-cp311-cp311-win_amd64.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/48.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/48.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/48.6 kB ? eta -:--:--\n",
      "     --------------------------------- ------ 41.0/48.6 kB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 48.6/48.6 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from optree) (4.12.2)\n",
      "Downloading optree-0.14.0-cp311-cp311-win_amd64.whl (300 kB)\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.4 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 30.7/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 30.7/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 30.7/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 30.7/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 30.7/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 30.7/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 30.7/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 30.7/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 30.7/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 41.0/300.4 kB 35.1 kB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 41.0/300.4 kB 35.1 kB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 41.0/300.4 kB 35.1 kB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 41.0/300.4 kB 35.1 kB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 41.0/300.4 kB 35.1 kB/s eta 0:00:08\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 61.4/300.4 kB 46.8 kB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 81.9/300.4 kB 35.0 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 92.2/300.4 kB 30.8 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 92.2/300.4 kB 30.8 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 92.2/300.4 kB 30.8 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 92.2/300.4 kB 30.8 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 92.2/300.4 kB 30.8 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 92.2/300.4 kB 30.8 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 92.2/300.4 kB 30.8 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 92.2/300.4 kB 30.8 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 92.2/300.4 kB 30.8 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 92.2/300.4 kB 30.8 kB/s eta 0:00:07\n",
      "   -------------- ------------------------ 112.6/300.4 kB 32.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------ 112.6/300.4 kB 32.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------ 112.6/300.4 kB 32.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------ 112.6/300.4 kB 32.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------ 112.6/300.4 kB 32.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------ 112.6/300.4 kB 32.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------ 112.6/300.4 kB 32.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------ 112.6/300.4 kB 32.8 kB/s eta 0:00:06\n",
      "   --------------- ----------------------- 122.9/300.4 kB 32.0 kB/s eta 0:00:06\n",
      "   --------------- ----------------------- 122.9/300.4 kB 32.0 kB/s eta 0:00:06\n",
      "   --------------- ----------------------- 122.9/300.4 kB 32.0 kB/s eta 0:00:06\n",
      "   --------------- ----------------------- 122.9/300.4 kB 32.0 kB/s eta 0:00:06\n",
      "   --------------- ----------------------- 122.9/300.4 kB 32.0 kB/s eta 0:00:06\n",
      "   --------------- ----------------------- 122.9/300.4 kB 32.0 kB/s eta 0:00:06\n",
      "   --------------- ----------------------- 122.9/300.4 kB 32.0 kB/s eta 0:00:06\n",
      "   --------------- ----------------------- 122.9/300.4 kB 32.0 kB/s eta 0:00:06\n",
      "   ------------------ -------------------- 143.4/300.4 kB 34.2 kB/s eta 0:00:05\n",
      "   ------------------ -------------------- 143.4/300.4 kB 34.2 kB/s eta 0:00:05\n",
      "   ------------------ -------------------- 143.4/300.4 kB 34.2 kB/s eta 0:00:05\n",
      "   ------------------ -------------------- 143.4/300.4 kB 34.2 kB/s eta 0:00:05\n",
      "   --------------------- ----------------- 163.8/300.4 kB 37.4 kB/s eta 0:00:04\n",
      "   --------------------- ----------------- 163.8/300.4 kB 37.4 kB/s eta 0:00:04\n",
      "   --------------------- ----------------- 163.8/300.4 kB 37.4 kB/s eta 0:00:04\n",
      "   --------------------- ----------------- 163.8/300.4 kB 37.4 kB/s eta 0:00:04\n",
      "   --------------------- ----------------- 163.8/300.4 kB 37.4 kB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 174.1/300.4 kB 37.9 kB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 174.1/300.4 kB 37.9 kB/s eta 0:00:04\n",
      "   ------------------------- ------------- 194.6/300.4 kB 41.5 kB/s eta 0:00:03\n",
      "   ------------------------- ------------- 194.6/300.4 kB 41.5 kB/s eta 0:00:03\n",
      "   ------------------------- ------------- 194.6/300.4 kB 41.5 kB/s eta 0:00:03\n",
      "   -------------------------- ------------ 204.8/300.4 kB 42.8 kB/s eta 0:00:03\n",
      "   -------------------------- ------------ 204.8/300.4 kB 42.8 kB/s eta 0:00:03\n",
      "   -------------------------- ------------ 204.8/300.4 kB 42.8 kB/s eta 0:00:03\n",
      "   -------------------------- ------------ 204.8/300.4 kB 42.8 kB/s eta 0:00:03\n",
      "   ----------------------------- --------- 225.3/300.4 kB 45.1 kB/s eta 0:00:02\n",
      "   ----------------------------- --------- 225.3/300.4 kB 45.1 kB/s eta 0:00:02\n",
      "   ----------------------------- --------- 225.3/300.4 kB 45.1 kB/s eta 0:00:02\n",
      "   ------------------------------- ------- 245.8/300.4 kB 48.3 kB/s eta 0:00:02\n",
      "   ------------------------------- ------- 245.8/300.4 kB 48.3 kB/s eta 0:00:02\n",
      "   ------------------------------- ------- 245.8/300.4 kB 48.3 kB/s eta 0:00:02\n",
      "   --------------------------------- ----- 256.0/300.4 kB 48.7 kB/s eta 0:00:01\n",
      "   ----------------------------------- --- 276.5/300.4 kB 52.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- --- 276.5/300.4 kB 52.1 kB/s eta 0:00:01\n",
      "   ------------------------------------- - 286.7/300.4 kB 53.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- - 286.7/300.4 kB 53.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- - 286.7/300.4 kB 53.3 kB/s eta 0:00:01\n",
      "   --------------------------------------  297.0/300.4 kB 53.8 kB/s eta 0:00:01\n",
      "   --------------------------------------- 300.4/300.4 kB 54.0 kB/s eta 0:00:00\n",
      "Installing collected packages: optree\n",
      "  Attempting uninstall: optree\n",
      "    Found existing installation: optree 0.12.1\n",
      "    Uninstalling optree-0.12.1:\n",
      "      Successfully uninstalled optree-0.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\HP\\\\anaconda3\\\\Lib\\\\site-packages\\\\~ptree\\\\_C.cp311-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade optree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73b8e33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.43.2)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/20/37/1f29af63e9c30156a3ed6ebc2754077016577c094f31de7b2631e5d379eb/transformers-4.49.0-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/44/69/d21eb253fa91622da25585d362a874fa4710be600f0ea9446d8d0217cec1/tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\HP\\\\anaconda3\\\\Lib\\\\site-packages\\\\~okenizers\\\\tokenizers.cp311-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9be683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ab19df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    #save grog api to my local environment\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "#from langchain.text_splitter import CharacterTextSplitter\n",
    "#from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5124d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"]=\"your api key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a628dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load PDF\n",
    "def load_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "197b1254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document\n",
    "file_path = \"NIPS-2017-attention-is-all-you-need-Paper.pdf\"\n",
    "document_text = load_pdf(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "529945b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2907, which is longer than the specified 1000\n",
      "Created a chunk of size 4253, which is longer than the specified 1000\n",
      "Created a chunk of size 1755, which is longer than the specified 1000\n",
      "Created a chunk of size 2448, which is longer than the specified 1000\n",
      "Created a chunk of size 3206, which is longer than the specified 1000\n",
      "Created a chunk of size 3544, which is longer than the specified 1000\n",
      "Created a chunk of size 3227, which is longer than the specified 1000\n",
      "Created a chunk of size 3327, which is longer than the specified 1000\n",
      "Created a chunk of size 2617, which is longer than the specified 1000\n",
      "Created a chunk of size 3089, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "# Split the text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_text(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8f56c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d954a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "add23271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.',\n",
       " 'Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2',\n",
       " 'Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21fb127",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2157ca36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_5584\\1837158031.py:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings=HuggingFaceEmbeddings()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4a2d07750c4f9e821097e78d734631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  96%|#########5| 419M/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfc087b5d5143fb909a74b28492d58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c4ecb11a564643b14980218200326f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd1b6f1fc904e96981f9e183d466f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9957df63ae444199e8bddc2f1ca28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a588e0a1a35a45ceb81e62a75e627e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings=HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec50270",
   "metadata": {},
   "source": [
    "# Putting in Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "332b7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##directory for chromadb\n",
    "##saved in same folder\n",
    "persistant_directory=\"doc_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5c1fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "363b0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(page_content=text) for text in documents]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea8d1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb=Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persistant_directory\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa3e2e",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb3bb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever= vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7453ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "##llm from groq\n",
    "\n",
    "\n",
    "llm=ChatGroq(\n",
    "    model='llama-3.3-70b-versatile',\n",
    "    temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9d33d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a QA Chain\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bbf22d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke the qa chain and get a response for user query\n",
    "\n",
    "\n",
    "query=\"What is the model architecture in the paper?\"\n",
    "response=qa_chain.invoke({\"query\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ebf036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the model architecture in the paper?', 'result': 'The model architecture in the paper is called the Transformer. It consists of an encoder and a decoder, both of which are composed of a stack of identical layers. \\n\\nThe encoder has 6 identical layers, each with two sub-layers: \\n1. Multi-head self-attention mechanism\\n2. Position-wise fully connected feed-forward network\\n\\nThe decoder also has 6 identical layers, each with three sub-layers: \\n1. Multi-head self-attention mechanism\\n2. Multi-head attention over the output of the encoder stack\\n3. Position-wise fully connected feed-forward network\\n\\nThe model uses residual connections around each sub-layer, followed by layer normalization. The output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. \\n\\nThe model also uses positional encoding to preserve the order of the sequence, as it does not use recurrence or convolution. The positional encoding is added to the input embeddings at the beginning of the encoder. \\n\\nThe model has a number of hyperparameters, including the number of layers (N), the dimensionality of the model (dmodel), the dimensionality of the feed-forward network (dff), the number of attention heads (h), and the dropout rate (Pdrop). The base model uses the following hyperparameters: N = 6, dmodel = 512, dff = 2048, h = 8, and Pdrop = 0.1.', 'source_documents': [Document(id='15faf320-345e-455f-9926-40e41c5051b0', metadata={}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'), Document(id='06d4a4bc-7209-44f9-bd44-e82b1e00649a', metadata={}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'), Document(id='6d5a86a2-d06a-46ea-9f13-4bf885b7e72b', metadata={}, page_content='MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'), Document(id='0e0af4bf-a328-43a0-870b-2446e21e549f', metadata={}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10')]}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d369d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model architecture in the paper is called the Transformer. It consists of an encoder and a decoder, both of which are composed of a stack of identical layers. \n",
      "\n",
      "The encoder has 6 identical layers, each with two sub-layers: \n",
      "1. Multi-head self-attention mechanism\n",
      "2. Position-wise fully connected feed-forward network\n",
      "\n",
      "The decoder also has 6 identical layers, each with three sub-layers: \n",
      "1. Multi-head self-attention mechanism\n",
      "2. Multi-head attention over the output of the encoder stack\n",
      "3. Position-wise fully connected feed-forward network\n",
      "\n",
      "The model uses residual connections around each sub-layer, followed by layer normalization. The output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. \n",
      "\n",
      "The model also uses positional encoding to preserve the order of the sequence, as it does not use recurrence or convolution. The positional encoding is added to the input embeddings at the beginning of the encoder. \n",
      "\n",
      "The model has a number of hyperparameters, including the number of layers (N), the dimensionality of the model (dmodel), the dimensionality of the feed-forward network (dff), the number of attention heads (h), and the dropout rate (Pdrop). The base model uses the following hyperparameters: N = 6, dmodel = 512, dff = 2048, h = 8, and Pdrop = 0.1.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb2cf180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(response[\"source_documents\"][0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "14b0dd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model architecture in the paper is called the Transformer. It consists of an encoder and a decoder, both of which are composed of a stack of identical layers. \n",
      "\n",
      "The encoder has 6 identical layers, each of which has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. \n",
      "\n",
      "The decoder also has 6 identical layers, each of which has three sub-layers: a multi-head self-attention mechanism, a multi-head attention mechanism over the output of the encoder stack, and a position-wise fully connected feed-forward network. \n",
      "\n",
      "The model uses residual connections around each of the sub-layers, followed by layer normalization. The output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. \n",
      "\n",
      "The model also uses positional encoding to inject information about the relative or absolute position of the tokens in the sequence, as it does not contain recurrence or convolution.\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "#invoke the qa chain and get a response for user query\n",
    "\n",
    "\n",
    "query=\"What is the model architecture in the paper?\"\n",
    "response=qa_chain.invoke({\"query\":query})\n",
    "print(response[\"result\"])\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541c98b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51453f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4c30e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddacfce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
