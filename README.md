# Doc-Q-A-withLLAMA3.1-RAG-DocQA-ChromaDB
This project implements a Retrieval-Augmented Generation (RAG) Document Q&A system using LLaMA, Groq, and ChromaDB. The system enables efficient document-based querying by retrieving relevant information from vectorized text chunks and generating context-aware responses.

**Features**
**LLama for Natural Language Understanding**: Utilizes Meta's LLaMA model for accurate and fluent responses.
**Groq for High-Speed Inference:** Runs on Groq’s ultra-fast inference engine for low-latency response generation.
**ChromaDB for Vector Search:** Efficient retrieval of document chunks using ChromaDB’s vector database.
**LangChain Integration:** Facilitates seamless pipeline management for document ingestion, retrieval, and response generation.

**Tech Stack**

Programming Language: Python
ML Models: llama-3.3-70b-versatile
Vector Database: ChromaDB
LLM Accelerator: Groq
Frameworks & Libraries: LangChain, Hugging Face Transformers
