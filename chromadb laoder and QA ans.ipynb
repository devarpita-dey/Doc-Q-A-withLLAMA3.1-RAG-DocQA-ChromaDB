{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ae0c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.0.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\HP\\\\anaconda3\\\\Lib\\\\site-packages\\\\~-kenizers\\\\tokenizers.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.43.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.29.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2022.7.9)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Obtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/65/8e/6d7d72b28f22c422cff8beae10ac3c2e4376b9be721ef8167b7eecd1da62/tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata\n",
      "  Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.7.22)\n",
      "Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1529295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9a5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"]=\"your api key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e5d8f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_18608\\1837158031.py:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings=HuggingFaceEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings=HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d464b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistant_directory=\"doc_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e09e634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the chromadb\n",
    "\n",
    "vectordb=Chroma(\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=persistant_directory\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77148534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever\n",
    "\n",
    "retriever= vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b77f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "##llm from groq\n",
    "\n",
    "\n",
    "llm=ChatGroq(\n",
    "    model='llama-3.3-70b-versatile',\n",
    "    temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f564fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a QA Chain\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "470b67ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model architecture in the paper is called the Transformer. It is a sequence transduction model that is based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "The Transformer model consists of an encoder and a decoder. The encoder is composed of a stack of 6 identical layers, each of which has two sub-layers: a multi-head self-attention sub-layer and a position-wise feed-forward network sub-layer. The decoder is also composed of a stack of 6 identical layers, each of which has three sub-layers: a multi-head self-attention sub-layer, a multi-head attention sub-layer over the output of the encoder stack, and a position-wise feed-forward network sub-layer.\n",
      "\n",
      "The model uses residual connections around each of the sub-layers, followed by layer normalization. The output of each sub-layer is computed as LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.\n",
      "\n",
      "The model also uses positional encoding to inject information about the order of the sequence into the model, since it does not have recurrence or convolution. The positional encoding is added to the input embeddings at the beginning of the encoder.\n",
      "\n",
      "The model has several key components, including:\n",
      "\n",
      "* Multi-head attention: The model uses multi-head attention in three different ways: encoder-decoder attention, self-attention in the encoder, and self-attention in the decoder.\n",
      "* Position-wise feed-forward network: The model uses a position-wise feed-forward network in each layer of the encoder and decoder.\n",
      "* Positional encoding: The model uses positional encoding to inject information about the order of the sequence into the model.\n",
      "* Embeddings and softmax: The model uses learned embeddings to convert the input tokens and output tokens to vectors, and a softmax function to convert the decoder output to predicted next-token probabilities.\n",
      "\n",
      "The model has several hyperparameters, including the number of layers (N), the dimensionality of the model (dmodel), the dimensionality of the feed-forward network (dff), the number of attention heads (h), and the dropout rate (Pdrop). The paper reports the results of several experiments with different hyperparameters, and finds that the model achieves state-of-the-art results on several machine translation tasks.\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "#invoke the qa chain and get a response for user query\n",
    "\n",
    "\n",
    "query=\"What is the model architecture in the paper?\"\n",
    "response=qa_chain.invoke({\"query\":query})\n",
    "print(response[\"result\"])\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe806e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
